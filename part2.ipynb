{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f2e980",
   "metadata": {},
   "source": [
    "# 1 set up basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers datasets accelerate peft sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f1eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('All_capped_keywords.csv')  \n",
    "# len(df) there are 91k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e8964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.loc[91000:91900, [\"abstract\", \"title\"]] # take an untouched subset for experimentation\n",
    "subset.to_csv(\"abstract_title.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009e571",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d2f0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavyarajasekaran/Desktop/Coding Projects/title generation/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (T5ForConditionalGeneration, T5TokenizerFast,\n",
    "                          DataCollatorForSeq2Seq, Trainer, TrainingArguments)\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"abstract_title.csv\")  \n",
    "df = df.dropna(subset=[\"abstract\",\"title\"]).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Small dev split\n",
    "n = len(df)\n",
    "split = int(0.9 * n) # 90% train, 10% eval\n",
    "train_df = df.iloc[:split] # first 90%\n",
    "eval_df  = df.iloc[split:] # last 10%\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df) \n",
    "eval_ds  = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d85ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/806 [00:00<?, ? examples/s]/Users/bhavyarajasekaran/Desktop/Coding Projects/title generation/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 806/806 [00:00<00:00, 6884.06 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6019.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Model & tokenizer\n",
    "model_name = \"t5-small\" # small model for lesser compute power\n",
    "tok = T5TokenizerFast.from_pretrained(model_name) # tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name) # model\n",
    "model.to(\"mps\")  # because I am running on apple silicon\n",
    "\n",
    "# for task framing\n",
    "PREFIX = \"summarize Abstract: \"\n",
    "\n",
    "# shortening for compute\n",
    "max_src_len = 512   \n",
    "max_tgt_len = 48    \n",
    "\n",
    "def preprocess(batch): # tokenize the inputs and labels\n",
    "    inputs = [PREFIX + a for a in batch[\"abstract\"]]  \n",
    "    targets = batch[\"title\"] \n",
    "    model_inputs = tok(inputs, max_length=max_src_len, truncation=True) # tokenize inputs\n",
    "    with tok.as_target_tokenizer(): # tokenize targets\n",
    "        labels = tok(targets, max_length=max_tgt_len, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"] # set as labels\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "eval_tok  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tok, model=model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e693a9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/trkc1vhj395bw5dw72jzyszr0000gn/T/ipykernel_6437/492662655.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/bhavyarajasekaran/Desktop/Coding Projects/title generation/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 03:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.365900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.197100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('t5_title_ft/final/tokenizer_config.json',\n",
       " 't5_title_ft/final/special_tokens_map.json',\n",
       " 't5_title_ft/final/spiece.model',\n",
       " 't5_title_ft/final/added_tokens.json',\n",
       " 't5_title_ft/final/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training args\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"t5_title_ft\", \n",
    "    num_train_epochs=3,         \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  # effective batch approx 16\n",
    "    learning_rate= 3e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tok,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"t5_title_ft/final\")\n",
    "tok.save_pretrained(\"t5_title_ft/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d15c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Abstract: Automatic taxonomy construction aims to build a categorization system without human efforts. Traditional textual pattern based methods extract hyponymy relation in raw texts. However, these methods us...\n",
      "True Title:  Coarse to Fine: Diffusing Categories in Wikipedia\n",
      "Prediction Title: Using diffusing Attributes from Wikipedia Infoboxes\n",
      "\n",
      "Abstract: This paper looks into the use of Information and Communication Technology (ICT) for Smart Sustainable Cities (SSC). It specifically points towards ICT's potential to help cities mitigate climate chang...\n",
      "True Title:  Reflections Regarding ICT and a Citizen-centric Future Path of Smart Sustainable Cities: AW4City 2018 Keynote\n",
      "Prediction Title: ICT for Smart Sustainable Cities\n",
      "\n",
      "Abstract: Ranking algorithms play a crucial role in online platforms ranging from search engines to recommender systems. In this paper, we identify a surprising consequence of popularity-based rankings: the few...\n",
      "True Title:  The few-get-richer: a surprising consequence of popularity-based rankings?\n",
      "Prediction Title: Results in Popularity-based Ranking\n",
      "\n",
      "Abstract: Predicting missing links between entities in a knowledge graph is a fundamental task to deal with the incompleteness of data on the Web. Knowledge graph embeddings map nodes into a vector space to pre...\n",
      "True Title:  Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models\n",
      "Prediction Title: The Combinaison of Relational and Structural Patterns in Knowledge Graph Embedding\n",
      "\n",
      "Abstract: Consumers today face too many reviews to read when shopping online. Presenting the most helpful reviews, instead of all, to them will greatly ease purchase decision making. Most of the existing studie...\n",
      "True Title:  Multi-Domain Gated CNN for Review Helpfulness Prediction\n",
      "Prediction Title: Multi-domain: Multi-domain Approach to Model Domain Relationships\n"
     ]
    }
   ],
   "source": [
    "# Inference: generate titles for new abstracts\n",
    "def generate_title(abstract: str, max_new_tokens=32, num_beams=4):\n",
    "    inp = tok(PREFIX + abstract, return_tensors=\"pt\", truncation=True, max_length=max_src_len) \n",
    "    inp = {k:v.to(\"mps\") for k,v in inp.items()} \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inp, max_new_tokens=max_new_tokens, num_beams=num_beams, length_penalty=0.8)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# demo\n",
    "for i in range(min(5, len(eval_df))):\n",
    "    abs_ = eval_df.iloc[i][\"abstract\"]\n",
    "    ref  = eval_df.iloc[i][\"title\"]\n",
    "    pred = generate_title(abs_)\n",
    "    print(f\"\\nAbstract: {abs_[:200]}...\")\n",
    "    print(f\"True Title:  {ref}\")\n",
    "    print(f\"Prediction Title: {pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
